\documentclass{tufte-handout}

\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}

\newcounter{example}
\newcounter{exc:first:proof}
\newenvironment{example}
{\refstepcounter{example}\begin{quote}
\textbf{Example \arabic{example}}}
{

$\square$\end{quote}}

\newenvironment{exercise}
{\refstepcounter{example}\begin{quote}
\textbf{Exercise \arabic{example}}}
{\end{quote}}

\newcommand{\RED}[1]{\textcolor{red}{#1}}
\newcommand{\TODO}[1]{\RED{TODO: #1}}

\newcounter{theExerciseCounter}

\title[You can Prove it!]{You can Prove it!\\
{\large A Practical Tutorial in Writing Proofs}}

\author{Elias Castegren \& Ellen Arvidsson}

\synctex=1

\begin{document}
\maketitle

\marginnote{This document was developed for the course Semantics
  of Programming Languages at Uppsala University. Please send any
  feedback to \texttt{elias.castegren@it.uu.se}. }

\tableofcontents

\section{Motivation}

The purpose of this document is to refresh your knowledge about
mathematical reasoning; in particular how to write \emph{proofs}.
%
The point of a proof is to present arguments that convince someone
(possibly yourself!) that something is true.
%
Proofs can have varying levels of formality, from completely
informal text\footnote{``Clearly $2 + 1 = 3$''.} to fully formal
reasoning from definitions and axioms\footnote{``Let natural
  numbers be either 0 or the successor $S~n$ of another natural
  number $n$. Let $0 + m = m$ and $(S~n)+m = S~(n+m)$. By the
  definition of $2$ and $1$, we have $2 + 1 = (S~(S~0)) + (S~0)$.
  By the definition of $+$, we have
  $(S~(S~0)) + (S~0) = S~((S~0) + (S~0)) = S~(S~(0 + (S~0))) =
  S~(S~(S~0)) = 3$ (by the definition of $3$). \vspace{5mm}}, and anything
in between.
%
How formal you should be depends on the context and who you are
trying to convince; if you are reasoning with a friend or yourself
you can probably be quite informal, but if you are designing a
railway scheduling system being very formal, so formal that the
proof can be checked by a computer\footnote{There is special kind of software called \emph{proof assistants}
  which checks every step you make in a proof, ensuring that you
  only take correct steps.}, can be a way of being very sure that
two trains never meet on the same track.
%
In this document we try to be pragmatic. We will write out and
motivate each step of a proof, but not require reasoning from
axioms every time.

This document should not be seen as a beginner's text book in
logic or even proofs -- the text assumes that you have a basic
understanding of both logic and proofs. The first goal of this
document is to have you brush up on this understanding and remind
you about concepts and terminology. A large part of doing proofs
is quite mechanical, but learning how to operate the machinery
requires practice. The second goal is therefore to provide you
with a set of practical tools that you can use when doing your own
proofs.
%
If you are taking the course Semantics of Programming Languages,
this document also sets the expectations of the level of formality
in seminars and assignments.

The rest of this tutorial has two main sections.
%
The first section covers how to translate informal statements into
formal ones -- that is, how to translate natural language into
mathematical symbols that clearly and unambiguously express what
you mean.
%
The second section covers techniques for proofs, sorted by the
kinds of constructs you might come across and how to handle them.
%
% TODO:
% There is also a bonus section that motivates the concept of
% structural induction\footnote{It is fully possible to \emph{use}
%   induction without understanding it formally, but the section is
%   included for the interested reader.}.
%
Each section comes with a couple of exercises. Suggested solutions
are provided at the end of this document.

\section{``What do you Mean by That?''}

Before you can even start to prove anything, you should understand
what every part of the thing you are proving means. In other
words, you need to know how to \emph{state} what it is you want to
prove in a clear way. We do this by translating informal
statements written in text to formal statements written using
mathematical symbols.
%
One of the points of this is to get rid of the ambiguity that is
prevalent in natural languages, for example in how we use
conjunctions like ``or'' when we speak. If you are asking someone
if they would like tea or coffee, you are implicitly expecting
them to pick one of them, and not both\footnote{While it is
  logically correct to answer ``yes'' to the question ``Would you
  like tea or coffee'', it is also quite obnoxious. Don't be that
  person. }. As you know, in logic there is a distinction between
``inclusive or'' and ``exclusive or'', each with their own
(clearly defined) semantics.

Once you get used to doing proofs you can afford to be less formal
in your formulations, but in order to learn the mechanics of
mathematical reasoning it helps to be explicit.
%
Sometimes an exercise or assignment will give you a formal
statement to begin with, but most times you will not be so lucky.

When dealing with proofs, we typically call a statement that can
be true or false (but not both) a
\emph{proposition}\footnote{Often we will use ``derivable''
  instead of ``true'', since we might be looking at what we can
  derive with a given set of rules rather than be concerned with
  the philosophical concept of truth. }. Here are three examples
of propositions:
\begin{itemize}
\item ``Three is less than two''
\item ``There is at least one natural number between five
and ten''
\item ``Bananas are yellow''
\end{itemize}

\noindent
Note that even though the first example is clearly not true, it is
still a proposition!
%
In order to write the third statement down with mathematical
symbols, we need a way to express the concepts of ``something
being a banana'' and ``something being yellow''. These are
examples of \emph{predicates}. Predicates can be thought of as
functions whose result is a proposition; we can write
$\mathit{banana}(x)$ or $\mathit{yellow}(x)$ for the propositions
that $x$ (whatever it is) is a banana or yellow respectively. Note
that $\mathit{yellow}$ is a predicate, while $\mathit{yellow}(x)$
is a proposition (assuming $x$ is defined).

\marginnote{\textbf{Glossary}\\
\begin{itemize}
\item A \emph{proposition} is a statement that is either true or false.
\item A \emph{predicate} is a function whose result is a proposition.
\item A \emph{relation} is a predicate which takes two or more arguments.
\end{itemize}
}

With these predicates, we can write the propositions formally as
follows:

\begin{itemize}
\item $3 < 2$
\item $\exists n \in \mathbb{N} .~ 5 < n ~\land~ n < 10$
\item $\forall b. \mathit{banana}(b) \implies \mathit{yellow}(b)$
\end{itemize}

\noindent
We are not claiming that these statements are necessarily
\emph{better} than the informal ones; one might even argue that
the informal ones are easier to understand. However, we have
gotten rid of some ambiguity from the informal statements.
%
For example, when we say that ``Bananas are yellow'', do we mean
that \emph{all} bananas are yellow or that \emph{there are}
bananas which are yellow\footnote{This distinction turns out to be
  important: only the latter interpretation is true since there
  are bananas which are red!}. When formalising the proposition,
we get rid of this ambiguity:

\begin{itemize}
\item ``All bananas are yellow'' -- $\forall b. \mathit{banana}(b) \implies \mathit{yellow}(b)$
\item ``Some bananas are yellow'' -- $\exists b. \mathit{banana}(b) \land \mathit{yellow}(b)$
\end{itemize}

\noindent
Note also that have not \emph{defined} how to identify a banana or
that an object is yellow, which may or may not be important for
what you want to do with these predicates. The important step for
now is that we have put a name on these properties\footnote{There
  is a \emph{symbolic representation} of the properties}.

Another thing to note is that we used (for example) the symbols
$<$, $2$ and $3$ without introducing them. This is because they
already have meanings that a reader can be expected to understand.
Indeed, the symbol $<$ is also a predicate, but one that takes two
arguments (note how $3 < 2$ is a proposition). A predicate that
takes two or more arguments is often called a \emph{relation}.

\begin{example}\label{ex:busdriver}
  There is a Swedish children's song (to the tune of ``Oh
  Tannenbaum'') called ``En busschauff\"{o}r'' whose lyrics let us
  know that ``A bus driver is a man with a happy
  mood''\footnote{Swedish children learn the logical concept of
    the \emph{contrapositive} from an early age. The song
    continues to state that ``if he does not have a happy mood,
    then he is no bus driver'':
    $\forall p. \lnot \mathit{happy}(p) \implies \lnot
    \mathit{busDriver}(p)$}. In order to formalise this
  proposition, we start by introducing three predicates:
  \begin{itemize}
  \item $\mathit{busDriver}(p)$ -- $p$ is a bus driver
  \item $\mathit{male}(p)$ -- $p$ identifies as male
  \item $\mathit{happy}(p)$ -- $p$ has a happy mood
  \end{itemize}
  The proposition of the song can now be formalised as ``for any
  person $p$, if $p$ is a bus driver, then $p$ is male and $p$ has
  a happy mood'':
  \[
    \forall p.
    \mathit{busDriver}(p) \implies
    \mathit{male}(p) ~\land~
    \mathit{happy}(p)
  \]
  Again, we can learn from this example is that it is perfectly
  possible to formalise propositions which are not true: most of
  us will have met bus drivers who do not identify as male or who
  we would not classify as happy.
\end{example}

\noindent
We will end this section with an example that is more connected to
computer science than the colour of bananas and the mood of bus
drivers.

\begin{example}\label{ex:filter}
  A list is either empty or consists of one element followed by
  the rest of the list. A list can be \emph{filtered} given some
  predicate $P$, such that elements that do not satisfy $P$ are
  discarded. One important property of filtering is that if an
  element for which $P$ holds is in a list, this element will also
  be in the list if it is filtered with $P$\footnote{This is a
    property we want, but it is not enough. Try to think about a
    definition of filtering that is wrong but which still
    satisfies this property. }.

  If we assume the existence of a $\mathit{filter}$ operation and
  a relation $\mathit{member}$ that holds if a given integer is in
  a given list, we can formalise the this property as a
  proposition:
  \[
    \forall P, x, l.~P(x) ~\land~ \mathit{member}(x, l)
    \implies
    \mathit{member}(x, \mathit{filter}(l, P))
  \]
  We will go ahead and formalise this with enough details so that
  we can actually prove that what we have said holds (although
  the proof will be given in a later section).

  We start by defining the syntax of lists, keeping the elements
  abstract\footnote{We could for example declare some set of
    possible list elements $\mathbb{E}$ and say that $x$ is some
    element of that set: $x \in \mathbb{E}$.}. We write this in
  Backus-Naur form, use $\epsilon$ to mean ``the empty list'' and
  separate elements of the list with semicolons:

  \[
      \mathit{list} ::= \epsilon ~|~ x; \mathit{list}
  \]

  \noindent
  We then continue to define the operation $\mathit{filter}$ which
  produces a filtered list given some predicate $P$:

  $\mathit{filter(l, P)} =
  \begin{cases}
    \epsilon & \text{if } l = \epsilon\\
    x; \mathit{filter}(l', P) & \text{if } l = x; l' \text{ and } P(x) \text{ holds}\\
    \mathit{filter}(l', P) & \text{if } l = x; l' \text{ and } P(x) \text{ does not hold}\\
  \end{cases}$

  \noindent
  Finally, we define the derivation rules for when an element is a
  member of a list; it is either the first element of the list, or
  it is a member of the rest of the list:

  \[
    \frac{}{~\mathit{member}(x, x; \mathit{list})~}
    \qquad
    \frac{~\mathit{member}(x, \mathit{list})~}{~\mathit{member}(x, y; \mathit{list})~}
  \]

\end{example}

\subsection{Exercises}

\begin{enumerate}
\item\label{ex:freshclean} The Outkast song ``So Fresh, So Clean''
  from the 2000 album Stankonia starts with the statement ``Ain't
  nobody dope as me, I'm just so fresh, so clean''. Formalise this
  statement, assuming this can be generalised so that any person
  who is both fresh and clean is more dope than anyone
  else\footnote{You can ignore the fact that this introduces a
    contradiction if there are two people who are both fresh and
    clean}. Start by introducing any predicates and functions you
  might need.
  % ∀p. fresh(p) ∧ clean(p) => ∀q. dopeness(q) < dopeness(p)

\item\label{ex:offside} The offside rule in soccer states that a player is in
  ``offside position'' if they are
  \begin{enumerate}
  \item in the opponents' half of the pitch;
  \item closer to the opponents' goal than the ball is; and
  \item closer to the opponents' goal than the second-closest
    opponent (the opponent closest to the goal is typically the
    goal keeper)
  \end{enumerate}

  Define the proposition ``player $p$ is in offside position''.
  For simplicity you may assume that the pitch is a
  one-dimensional line and that both the ball and the players are
  distinct points on this line. Write your proposition as an
  equivalence, i.e. $\forall p. \mathit{offside}(p) \iff \dots$,
  where $\dots$ is your definition. You may want to introduce
  helper functions for getting different positions on the pitch or
  for getting sets of players\footnote{\textbf{Hint:} Set
    comprehensions may also come in handy. For example, the set of
    all even natural numbers can be written as
    $\{n ~|~ n \in \mathbb{N} \land n~mod~2 = 0\}$}.
  % ∀p. offside(p) <=>
  %     pos(p) < goal(p)/2 ∧
  %     pos(p) < ball ∧
  %     forall q. q ∈ {q | q ∈ opponents(p) ∧ pos(q) > min(positions(opponents(p)))} ==> pos(p) < pos(q)
\setcounter{theExerciseCounter}{\value{enumi}}
\end{enumerate}


\section{``Oh yeah? Prove it!''}

How does one ensure that only correct steps are taken in a proof?
This is varies depending on how formal you want to be in your
proof. In the end, the point of a proof is convincing someone that
a proposition holds. Sometimes the right level of formality is
just using natural language, and sometimes the right level is to
derive every step from logical axioms.
%
Here, we will take a middle route where we write all propositions
formally, but write our reasoning using natural language.
%
We will cover how to work with different logical constructs that
you might come across while working on a proof. We also cover two
proof techniques: proof by induction and proof by contradiction.

All proofs have a \emph{goal}, the thing you are trying to prove
holds. You also have a set of \emph{assumptions}, things that you
are assuming holds already\footnote{Sometimes assumptions are
  called \emph{hypotheses}. Note that we use the same terminology
  even for things that are obviously true (like $1=1$), even
  though it seems to imply that we might not be sure.}. Both the
goal and the assumptions are propositions as seen in the previous
section. The process of conducting a proof is using logical steps
starting from the assumptions and hopefully arriving at the goal.

The mechanical steps involved in handling the different logical
constructs covered here are different depending on whether the
construct appears as a goal or in an assumption. Note also that
there is no algorithm for how to prove anything -- this section
gives you tools that you can apply when writing proofs.


\subsection{Implication}

One of the most basic logical constructs is the implication
$A \!\!\implies\!\! B$. It states that if we know $A$, then we can also
know $B$\footnote{An implication $A \implies B$ is like a function
  whose argument is (a proof of) $A$ and whose result is (a proof
  of) $B$.}. There are two important things to note. Firstly,
there is nothing that says that we \emph{have to} use $A$ when
proving $B$, just that we are allowed to. Secondly, if we happen
to know that $A$ does \emph{not} hold, this does not say anything
about whether $B$ holds or not.

\paragraph{In the Goal}
If our goal is an implication $A \implies B$, we are asked to show
that we can derive $B$ starting from $A$ (and whatever other
assumptions we may have). We do this by adding $A$ to our set of
assumptions and proceeding to show $B$.

\paragraph{As an Assumption}
If we have an implication $A \implies B$ as an assumption (or as a
previously proved proposition), we may derive $B$ if we already
know $A$.

\paragraph{Examples}

\begin{itemize}
\item For any $n$, show $n < 5 \implies n + 2 < 7$.\\
  \textbf{Proof}
  \begin{compactenum}
  \item We start by assuming $n < 5$. We need to show $n + 2 < 7$.
  \item By adding two on both sides we get $n + 2 < 5 + 2$.
  \item By basic arithmetics, $5 + 2 = 7$, giving $n + 2 < 7$. $\square$
  \end{compactenum}
\item For any $n$, show $n < 5 \implies 5 < 7$.\\
  \textbf{Proof}
  \begin{compactenum}
  \item We start by assuming $n < 5$. We need to show $5 < 7$,
    which holds trivially\footnote{Note that we didn't need to use
      the assumption here, but we still ``peel it off'' in order
      to reach the right-hand side.}. $\square$
  \end{compactenum}
\item Assuming $A \implies B$ and $B \implies C$, show $A \implies C$.\\
  \textbf{Proof}
  \begin{compactenum}
  \item We start by assuming $A$. We need to show $C$.
  \item By the first assumption\footnote{With only two
      assumptions, we can get away with just referring to them as
      ``the first'' and ``the second''. For larger proofs it is a
      good idea to give names to your different assumptions. },
    having $A$ gives us $B$.
  \item By the second assumption, having $B$ gives us $C$. $\square$
  \end{compactenum}
\end{itemize}

A special case of proofs involving implications occur when the
left-hand side of an implication does not hold. Remember that for
$A \implies B$ we are only asked to show $B$ \emph{if} $A$ holds.
If $A$ does not hold, we don't have to do anything; we say that
the proposition \emph{holds vacuously}. Similarly, if one of our
assumptions turns out to be false, we can conclude the proof,
since we have ended up in a situation that cannot
happen\footnote{This is known as the \emph{principle of
    explosion}, or by the Latin phrase \emph{Ex falso quodlibet},
  ``from falsehood [follows] anything''.}. This also relates to
proof by contradiction, which is covered below.


\subsection{Equivalence}

An equivalence $A \iff B$ says that $A$ is true if \emph{and only
  if} $B$ is true. It is equivalent (no pun intended) to having
two implications, $A \implies B$ and $B \implies A$.

\paragraph{In the Goal}
If our goal is an equivalence $A \iff B$, we need to show both
directions of the equivalence. In other words, we need to show
both $A \implies B$ and $B \implies A$.

\paragraph{As an Assumption}
If we have an equivalence $A \iff B$ as an assumption, we may use
this assumption to show $B$ if we already have $A$, or to show $A$
if we already have $B$\footnote{Again, it is as if we had two
  assumptions, one for each direction of the equivalence}.

\paragraph{Example}

\begin{itemize}
\item For any $n \leq 0$, show that $n < 10 \iff n^2 < 100$.\\
  \textbf{Proof}
  \begin{compactenum}
  \item We start by showing the first direction $n < 10 \Longrightarrow n^2 < 100$:
    \begin{compactenum}
    \item We start by assuming $n < 10$. We need to show $n^2 < 100$.
    \item By squaring both sides, we get $n^2 < 10^2$.
    \item By basic arithmetics, $10^2 = 100$, giving $n^2 < 100$.
    \end{compactenum}
  \item Next we show the second direction $n^2 < 100 \implies n < 10$:
    \begin{compactenum}
    \item We start by assuming $n^2 < 100$. We need to show $n < 10$.
    \item By taking the square root of both sides, we get $\sqrt{n^2} < \sqrt{100}$.
    \item By basic algebra, $\sqrt{n^2} = n$,
    \item By basic arithmetics, $\sqrt{100} = 10$, giving $n < 10$.
    \end{compactenum}
    $\square$
  \end{compactenum}
\end{itemize}


\subsection{Conjunctions and Disjunctions}

\marginnote{\textbf{Connectives}\\
  Implication, equivalence, conjunctions and disjunctions are all
  examples of logical \emph{connectives}. A connective is a
  predicate whose arguments are one or more propositions. Another
  connective that you are likely to come across is negation
  $\lnot A$.}

Conjunctions (written $\land$) and disjunctions ($\lor$) are the
fancy names of logical \emph{and} and logical \emph{or}.
%
A conjunction $A \land B$ is true if both $A$ and $B$ are true.
%
A disjunction $A \lor B$ is true if at least one of $A$ and $B$ are true.

\paragraph{In the Goal}
If our goal is a conjunction $A \land B$, we need to show both $A$
and $B$. We can do this by splitting our proof into two
sub-proofs, just as we did for the equivalence proof above.
%
If our goal is a disjunction $A \lor B$, we need to show either
$A$ or $B$ but it is enough to show one of them.

\paragraph{As an Assumption}
If we have a conjunction $A \land B$ as an assumption, we get to
assume both $A$ and $B$. If we have a disjunction $A \lor B$ as an
assumption, since either $A$ or $B$ must be true, we can split our
proof into two sub-proofs, one where we assume only $A$ and one
where we assume only $B$\footnote{Note that we are not assuming
  that if $A$ is true, then $B$ is false. It is possible that they
  are both true at the same time. }.


\paragraph{Examples}

\begin{itemize}
\item Show commutativity of conjunction, i.e. $A \land B \implies B \land A$.\\
  \textbf{Proof}
  \begin{compactenum}
  \item We start by assuming $A \land B$. We need to show $B \land A$
  \item Our assumption lets us assume $A$ and $B$.
  \item We can show $B \land A$ by showing $B$ and $A$ separately,
    both of which hold by assumption. $\square$
  \end{compactenum}
\item Show commutativity of disjunction, i.e. $A \lor B \implies B \lor A$.\\
  \textbf{Proof}
  \begin{compactenum}
  \item We start by assuming $A \lor B$. We need to show $B \lor A$.
  \item We proceed by cases over the assumption $A \lor B$:
    \begin{compactenum}
    \item If $A$ holds, then that is enough to show $B \lor A$.
    \item If $B$ holds, then that is enough to show $B \lor A$.
    \end{compactenum}
  \end{compactenum}
  $\square$
\end{itemize}

\subsection{Quantifiers}

Quantifiers are used to give names to things for some scope.
The two most common quantifiers in logic are ``for all''
($\forall$), used to introduce a name that represents all
instances of something, and ``exists'' ($\exists$), used to
introduce a name that represents a particular but unspecified
instance of something.

\paragraph{In the Goal}
If our goal has the form $\forall x. P(x)$, where $P(x)$ can be any
proposition mentioning $x$\footnote{Or not. There is no rule that
  requires $x$ to be mentioned, but if it isn't mentioned there is
  no point in the quantifier being there.}, we can assume that we
have \emph{some} $x$ and proceed by showing that $P$ holds for
that $x$. Note that we can pick any name for $x$ (assuming we
update $P$ with the new name), which may be necessary if we
already have another $x$ in scope.
%
If our goal has the form $\exists x. P(x)$, we need to provide
some concrete value for which $P$ holds (or derive a contradiction
from our assumptions).

\paragraph{As an Assumption}
If we have an assumption $\forall x. P(x)$, we are allowed to
assume that $P$ holds for \emph{any} $x$ that appears in our
assumptions\footnote{In practice, there will almost always be some
  kind of precondition on $x$, for example that it is natural
  number: $\forall x\in\mathbb{N}.~P(x)$}.
%
If we have an assumption $\exists x. P(x)$, we are allowed to
assume that we have some concrete instance $x$ for which $P$
holds.

\paragraph{Examples}
\begin{itemize}
\item Show
  $\forall n\in\mathbb{N}. 0 < n \implies \exists m\in\mathbb{N}.
  m < n$.\\
  \textbf{Proof}
  \begin{compactenum}
  \item We start by assuming that we have some natural number $n$
    such that $0 < n$. We need to show
    $\exists m\in\mathbb{N}. m < n$.
  \item We pick $0$ as a concrete value. We need to show $0 < n$
    which holds by assumption. $\square$
  \end{compactenum}

  \marginnote{\textbf{Duality}\\
    Note how the two quantifiers behave sort of like opposites: we
    are allowed to assume the existence of a value when we have a
    $\forall$ in the goal or when we have an $\exists$ in our
    assumptions, but we have to provide a value when we have a
    $\forall$ in our assumptions or an $\exists$ in our goal. This
    is an example of a \emph{duality}, a deep mathematical
    connection that often pops up in formal logic. }

\item Show
  $\forall n\in\mathbb{N}. \exists m\in\mathbb{N}. n = m + 1 \implies n \neq 0$.\\
  \textbf{Proof}
  \begin{compactenum}
  \item We start by assuming that we have some natural number $n$
    and that $\exists m\in\mathbb{N}. n = m + 1$. We need to show
    $n \neq 0$.
  \item We assume that we have some natural number $m$ such that
    $n = m + 1$.
  \item We need to show $m + 1 \neq 0$, which holds trivially
    since $m$ cannot be smaller than $0$, the smallest natural
    number. $\square$
  \end{compactenum}
\end{itemize}


\subsection{Proof by Induction}

Some kinds of values can only take on a finite number of forms.
This allows us to do proofs by enumeration. For example, if we
wanted to show that $A \implies A \lor B$ we can just write out
the truth table for all possible values of $A$ and $B$ and see
that the result is always true\footnote{In propositional logic,
  the \emph{theorem of completeness} states that if all
  enumerations of truth values result in a proposition being true,
  then there is also a proof of that proposition.}:

\begin{tabular}{ccc}
  $A$&$B$&$A \implies A \lor B$\\
  \hline
  $F$ & $F$ & $F \implies F \lor F \equiv T$\\
  $F$ & $T$ & $F \implies F \lor T \equiv T$\\
  $T$ & $F$ & $T \implies T \lor F \equiv T$\\
  $T$ & $T$ & $T \implies T \lor T \equiv T$\\
\end{tabular}

However, a lot of the time the things we reason about can be
arbitrarily large\footnote{Sometimes even infinite, but we try to
  avoid that whenever possible since infinities are weird...}. For
example, if we want to prove something about natural numbers we
will not be able to list all of them. The standard technique for
dealing with this is doing a proof by induction.

If we want to prove some property $P$ about natural numbers, a
proof by induction is done in two steps: the \emph{base case},
where we show that $P$ holds for the number $0$, and the
\emph{inductive case} where we show that $P$ holds for $k + 1$,
assuming $P$ holds for some number $k$.
%
The intuition is that the base case gives us $P(0)$, and then the
inductive case first gives us $P(1)$ (for $k = 0$), and then
$P(2)$ (for $k = 1$), and so on up to any number\footnote{You can
  think of an inductive proof as defining a \emph{procedure} for
  how to prove the property for any given number.}.
%
The assumption that $P$ holds for some $k$ in the inductive case
is known as the \emph{induction hypothesis}.

\begin{example}\label{ex:distr}
  Show that multiplication distributes over addition:
  $\forall n,a,b.~n\cdot(a+b) = n\cdot a + n\cdot b$.

  \textbf{Proof}\\
  We assume that we have some natural numbers $n$, $a$ and $b$.
  We need to show $n\cdot(a+b) = n\cdot a + n\cdot b$.
  %
  We proceed by induction over $n$\footnote{It is always important
    to state which natural number you are doing induction over as
    there is often more than one number involved. }.
  \begin{itemize}
  \item \textbf{Base Case}: $n = 0$. Need to show $0 \cdot (a + b) = 0 \cdot a + 0 \cdot b$
    \begin{compactenum}
    \item By arithmetics, we have $0 \cdot (a + b) = 0$.
    \item Similarly, we have $0 = 0 \cdot a + 0 \cdot b$, thus
      $0 \cdot (a + b) = 0 \cdot a + 0 \cdot b$.
    \end{compactenum}
  \item \textbf{Inductive Case}: $n = k + 1$. Need to show
    $(k + 1) \cdot (a + b) = (k + 1) \cdot a + (k + 1) \cdot b$\\
    \textit{Induction Hypothesis}: $k \cdot (a + b) = k \cdot a + k \cdot b$
    \begin{compactenum}
    \item By the definition of multiplication, we have
      $(k + 1) \cdot (a + b) = k \cdot (a + b) + (a + b)$.
    \item By the induction hypothesis we have
      $k \cdot (a + b) + (a + b) = k \cdot a + k \cdot b + (a + b)$.
    \item By the definition of multiplication, we have
      $k \cdot a + k \cdot b + (a + b) = (k + 1) \cdot a + (k + 1)
      \cdot b$, thus
      $(k + 1) \cdot (a + b) = (k + 1) \cdot a + (k + 1) \cdot b$.
    \end{compactenum}
  \end{itemize}
\end{example}

\noindent
Note how the goal changes in the different cases. To make this
explicit above we repeat ``need to show'' in the different cases
so there's less to remember implicitly.
%
We make clear when we start our induction proof and which case we
are on. For each step of the proof, we do something that brings us
closer to our goal and write why this step is motivated. Ideally,
the last step of every (sub) proof contains the exact symbols
listed as ``need to show'' at the start.

You have probably seen induction proofs used for natural numbers
before, but the technique generalises to anything that has an
\emph{inductive structure}, that is anything defined in terms of
smaller versions of itself. This includes trees, lists, programs,
\emph{etc}.
%
For example, recall the definition of lists from
Example~\ref{ex:filter}:
\[
  \mathit{list} ::= \epsilon ~|~ x; \mathit{list}
\]
We can prove some property $P$ of lists by using \emph{structural
  induction}. We start by showing the base case, that $P$ holds
for the empty list $\epsilon$, and then we show the inductive
case, that $P$ holds for some arbitrary but non-empty list
$x;\mathit{list}$, assuming that $P$ holds for the sublist
$\mathit{list}$.
%
The intuition is the same as for induction over numbers: the base
case gives us $P(\epsilon)$, and then the inductive case gives us
$P(x;\epsilon)$ (for any $x$ and for $\mathit{list} = \epsilon$),
and then $P(y;x;\epsilon)$ (for any $y$ and for
$\mathit{list} = x;\epsilon$), and so on up to a list of arbitrary
length\footnote{The similarities become even more apparent when
  you consider Peano's definition of natural numbers $n$:
  $n ::= 0 ~|~ S~n$. A natural number is either zero, or the
  successor of another natural number. }.

\begin{example}\label{ex:filterproof}
  In Example~\ref{ex:filter} we formalised a proposition relating
  to filtering of lists that we will now prove:
  \[
    \forall x, l.~P(x) ~\land~ \mathit{member}(x, l)
    \implies
    \mathit{member}(x, \mathit{filter}(l, P))
  \]

  For convenience we also list the definition of $\mathit{filter}$
  and the derivation rules of $\mathit{member}$ again:

  $\mathit{filter(l, P)} =
  \begin{cases}
    \epsilon & \text{if } l = \epsilon\\
    x; \mathit{filter}(l', P) & \text{if } l = x; l' \text{ and } P(x) \text{ holds}\\
    \mathit{filter}(l', P) & \text{if } l = x; l' \text{ and } P(x) \text{ does not hold}\\
  \end{cases}$

  \[
    \frac{}{~\mathit{member}(x, x; \mathit{list})~}
    \qquad
    \frac{~\mathit{member}(x, \mathit{list})~}{~\mathit{member}(x, y; \mathit{list})~}
  \]

  We start by assuming we have some element $x$ and list $l$ such
  that $P(x)$ and $\mathit{member}(x, l)$\footnote{Here we are
    introducing quantified variables $x$ and $l$, assuming the
    left-hand side of the implication and splitting the
    conjunction $P(x) ~\land~ \mathit{member}(x, l)$ in a single
    step.}. We need to show
  $\mathit{member}(x, \mathit{filter}(l, P))$. We proceed by
  structural induction over $l$.
  \begin{itemize}
  \item \textbf{Base Case}: $l = \epsilon$. Need to show $\mathit{member}(x, \mathit{filter}(\epsilon, P))$\\
    \textit{Assumption 1}: $\mathit{member}(x, \epsilon)$\\
    \textit{Assumption 2}: $P(x)$\\
    \begin{compactenum}
    \item By \textit{Assumption 1}, we have
      $\mathit{member}(x, \epsilon)$. But this is impossible,
      since both derivation rules of $\mathit{member}$ requires a
      non-empty list. The base case thus holds vacuously.
    \end{compactenum}
  \item \textbf{Inductive Case}: $l = y; l'$. Need to show
    $\mathit{member}(x, \mathit{filter}(y; l', P))$\\
    \textit{Induction Hypothesis}\footnote{Note that the induction
      hypothesis turns into an implication when we have
      assumptions that mention the variable we are doing induction
      over. If we didn't do this, we could prove things that are
      not true. }:
    $\mathit{member}(x, l') \implies \mathit{member}(x,
    \mathit{filter}(l', P))$\\
    \textit{Assumption 1}: $\mathit{member}(x, y; l')$\\
    \textit{Assumption 2}: $P(x)$\\
    There are two rules that can be used for deriving
    $\mathit{member}(x, y; l')$. We proceed by cases over
    \textit{Assumption 1}:
    \begin{itemize}
      \item \textbf{Case 1}: $x = y$. Need to show
        $\mathit{member}(x, \mathit{filter}(x; l', P))$.
        \begin{compactenum}
        \item By the definition of $\mathit{filter}$ (case 2,
          since we have $P(x)$ from \textit{Assumption 2}) we have
          $\mathit{filter}(x; l', P) = x; \mathit{filter}(l', P)$.
        \item We have
          $\mathit{member}(x, x; \mathit{filter}(l', P))$ by the
          first derivation rule of $\mathit{member}$.
        \end{compactenum}
      \item \textbf{Case 2}: $\mathit{member}(x; l')$
        (\textit{Assumption 3}). Need to show
        $\mathit{member}(x, \mathit{filter}(y; l', P))$.
        \begin{compactenum}
        \item By the definition of $\mathit{filter}$ there are two
          possible values of $\mathit{filter}(y; l', P)$ depending
          on whether $y$ is removed or not.
        \item If $y$ is removed we have
          $\mathit{member}(x, \mathit{filter}(l', P))$ by the
          induction hypothesis with \textit{Assumption 3}.
        \item If $y$ is not removed, we have
          $\mathit{member}(x, y; \mathit{filter}(l', P))$ by the
          second derivation rule of $\mathit{member}$, getting the
          premise from the induction hypothesis with
          \textit{Assumption 3}.
        \end{compactenum}
      \end{itemize}
  \end{itemize}
\end{example}

Most parts of a proof by induction can be done mechanically
without thinking. The creative parts is choosing which variable to
do induction over and figuring out which steps to take in order to
get to the thing you have to prove. As long as we have a clear
structure, filling in the cases, assumptions, the induction
hypothesis and what the current goal is should be
automatic\footnote{That is not to say that it is not something
  that requires practice! }.


\subsection{Proof by Contradiction}

Another way (some would say \emph{the} other way) to prove that
something holds for all values of some infinite set (like the
natural numbers) is to show that it cannot \emph{not} be the case.
We typically accept that all propositions are either true or
false\footnote{This is known as the \emph{law of excluded middle};
  there is no truth value between true and false. }, and if we can
show that a proposition is not false then it must be true. This is
known as a proof by contradiction.

A proof by contradiction of some property $P$ starts out by
assuming $\lnot P$, i.e. that $P$ does not hold, and then tries to
derive a contradiction, i.e. an assumption that is false. If we
can derive a contradiction, our assumption of $\lnot P$ must have
been wrong, thus it \emph{must} be the case that $P$ holds.
%
It is not always obvious when a proof by contradiction is the
right choice of method, but it is something that is useful to have
in one's tool box. Personally, I reach for it when something
involves infinity.

\begin{example}
  A classic proof by contradiction is showing that there is an
  infinite number of prime numbers. Recall that a number is prime
  if it is only divisible by itself and 1, and that all numbers
  which are not prime have factors which are prime. As a
  technicality, also recall that if $n$ is a factor of both $a$
  and $b$, then it is also a factor of the difference of $a$ and
  $b$\footnote{Assume $a$ is the larger of the two numbers $a$ and
    $b$. If $n$ divides $a$ and $b$, then $\frac{a}{n}$ and
    $\frac{b}{n}$ are integers. We can derive the equality
    $a - b = n\frac{a}{n} - n\frac{b}{n} = n(\frac{a}{n} -
    \frac{b}{n})$, which is clearly divisible by $n$.}.

  Let us assume that there is actually a finite number of prime
  numbers, and let us list all of them as $p_1, p_2, \dots,p_n$.
  Let $P = p_1\cdot p_2\cdots p_n$ be the product of all primes
  and consider the number $P + 1$. We know that $P + 1$ is not a
  prime number (we just listed all of them!), so $P + 1$ must have
  a prime factor $p$. Since we just listed all of the primes, $p$
  must be one of $p_1, p_2, \dots,p_n$, meaning it is also a prime
  factor of $P$. Since $p$ divides both $P$ and $P + 1$, it must
  also divide $P + 1 - P = 1$. However, no prime number divides
  $1$, so $p$ cannot be one of $p_1, p_2, \dots,p_n$. This
  contradicts our assumption that $p_1, p_2, \dots,p_n$ contains
  all prime numbers, therefore the prime numbers must be
  infinitely many.
\end{example}

\subsection{Exercises}

\begin{enumerate}
\setcounter{enumi}{\value{theExerciseCounter}}
\setcounter{exc:first:proof}{\value{enumi}}
\item
  Show that conjunction distributes over disjunction:\\
  $\forall A~B~C. ~A \land (B \lor C) \iff (A \land B) \lor (A
  \land C)$. You could do this by building a truth table, but try
  to do it using a step-by-step proof.
\item Use induction to show that
  $(a + b)(c + d) = ab + ad + bc + bd$ for any $a$, $b$, $c$ and
  $d$\footnote{\textbf{Hint}: You may want to use the property
    that we proved in Example~\ref{ex:distr} as part of your
    proof.}.
\item The length of a list can be defined as follows:
  \[
  \mathit{length}(l) =
  \begin{cases}
    0 & \text{if } l = \epsilon\\
    1 + \mathit{length}(l')& \text{if } l = x; l'\\
  \end{cases}
  \]
  The concatenation of two lists can be defined as follows:
  \[
  \mathit{concat}(l_1, l_2) =
  \begin{cases}
    l_2 & \text{if } l = \epsilon\\
    x; \mathit{concat}(l_1', l_2) & \text{if } l_1 = x; l_1'\\
  \end{cases}
  \]
  Show that the length of the concatenation of two lists equals
  the sum of their individual lengths using structural induction.
\item In Example~\ref{ex:filterproof} we showed that a filtered
  list keeps all elements that satisfy the given predicate.
  Another expected property of filtering is that a filtered list
  only contains elements that satisfies the predicate. Formalise
  this statement as a proposition and prove that it holds.
\item Show that filtering the concatenation of two lists results
  in the concatenation of the filtered versions of the individual
  lists.
\end{enumerate}

% \section{Bonus: Motivation for Why Structural Induction Works}

% - Överkurs om induktion

\section{Suggested Solutions}

Some suggested solutions are omitted before the seminar.

\subsection{``What do you Mean by That?''}

\begin{enumerate}
\item If someone is both fresh and clean then they are more dope
  than everyone else. We introduce predicates $\mathit{fresh}$ and
  $\mathit{clean}$, as well as a function $\mathit{dopeness}$ that
  takes a person $p$ to their dopeness level (unit unspecified).
  We use the ``for all'' quantifier to introduce names $p$ and $q$
  for different people:
  \[
  \forall p.~\mathit{fresh}(p) \land \mathit{clean}(p) \implies
  \forall q.~\mathit{dopeness}(q) < \mathit{dopeness}(p)
  \]
  Note that having both quantifiers in the beginning would be an
  equivalent statement:
  \[
  \forall p.\forall q.~\mathit{fresh}(p) \land \mathit{clean}(p) \implies
  \mathit{dopeness}(q) < \mathit{dopeness}(p)
  \]
\end{enumerate}

\subsection{``Oh Yeah? Prove it!''}

\begin{enumerate}
\setcounter{enumi}{\value{exc:first:proof}}
\item We want to show
  $\forall A~B~C. ~A \land (B \lor C) \iff (A \land B) \lor (A
  \land C)$. We start by assuming we have some propositions $A$,
  $B$ and $C$ and proceed to show
  $A \land (B \lor C) \iff (A \land B) \lor (A \land C)$. We
  proceed by cases over the two directions:
  \begin{itemize}
  \item[\textbf{Case 1:}] $A \land (B \lor C) \implies (A \land B) \lor (A \land C)$\\
    %
    We start by assuming $A \land (B \lor C)$. We need to
    show $(A \land B) \lor (A \land C)$.
    %
    From our assumption we get $A$ and $B \lor C$. We proceed by
    cases over $B \lor C$:
    \begin{enumerate}
    \item If $B$ holds then we can show that $A \land B$ holds
      since both $A$ and $B$ hold.
    \item If $C$ holds then we can show that $A \land C$ holds
      since both $A$ and $C$ hold.
    \end{enumerate}

  \item[\textbf{Case 2:}]
    $(A \land B) \lor (A \land C) \implies A \land (B \lor C)$\\
    %
    We start by assuming $(A \land B) \lor (A \land C)$. We need
    to show $A \land (B \lor C)$. We proceed by cases over
    $(A \land B) \lor (A \land C)$:
    \begin{enumerate}
    \item If $(A \land B)$ holds, then $A$ and $B$ hold. Now $A$
      holds by assumption and from $B$ we can show $B \lor C$.
    \item If $(A \land C)$ holds, then $A$ and $C$ hold. Now $A$
      holds by assumption and from $C$ we can show $B \lor C$.
    \end{enumerate}
  \end{itemize}

\end{enumerate}

\end{document}

\TODO{Put actual solutions here for now}